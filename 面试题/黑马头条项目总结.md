# 黑马头条项目就业总结

## 目标

- 能够描述项目概述
- 能够描述项目中的核心业务
- 能够描述项目中的核心技术点

# 1 黑马头条项目介绍

## 1.1 项目概述

​		随着智能手机的普及，人们更加习惯于通过手机来看新闻。由于生活节奏的加快，很多人只能利用碎片时间来获取信息，因此，对于移动资讯客户端的需求也越来越高。黑马头条项目正是在这样背景下开发出来。黑马头条项目采用当下火热的微服务+大数据技术架构实现。本项目主要着手于获取最新最热新闻资讯，通过大数据分析用户喜好精确推送咨询新闻

![1561345993151](img/1-1.png)

![1561346047190](img/1-1-1.png)



黑马头条项目是对在线教育平台业务进行大数据统计分析的系统。碎片化、切换频繁、社交化和个性化现如今成为人们阅读行为的标签。黑马头条对海量信息进行搜集，通过系统计算分类，分析用户的兴趣进行推送从而满足用户的需求。

![1561346170460](img/1-2-1.png)

- 项目：泛指黑马头条整个项目或某一项目模块
- 工程：泛指黑马头条某一项目的源码工程

- 用户：泛指黑马头条APP用户端用户
- 自媒体人：泛指通过黑马自媒体系统发送文章的用户
- 管理员：泛指黑马头条管理系统的使用用户

## 1.2 架构介绍

### 1.2.1 功能架构

![1561349237384](img\2-1.png)

APP主要功能大纲

- 频道栏：用户可以通过此功能添加自己感兴趣的频道，在添加标签时，系统可依据用户喜好进行推荐
- 文章列表：需要显示文章标题、文章图片、评论数等信息，且需要监控文章是否在APP端展现的行为

- 搜索文章：联想用户想搜索的内容，并记录用户的历史搜索信息
- 个人中心：用户可以在其个人中心查看收藏、关注的人、以及系统设置等功能
- 查看文章：用户点击文章进入查看文章页面，在此页面上可进行点赞、评论、不喜欢、分享等操作；除此之外还需要收集用户查看文章的时间，是否看我等行为信息

- 实名认证：用户可以进行身份证认证和实名认证，实名认证之后即可成为自媒体人，在平台上发布文章
- 注册登录：登录时，验证内容为手机号登录/注册，通过手机号验证码进行登录/注册，首次登录用户自动注册账号。

WEMEDIA功能大纲

- 内容管理：自媒体用户管理文章页面，可以根据条件进行筛选，文章包含草稿、已发布、未通过、已撤回状态。用户可以对文章进行修改，上/下架操作、查看文章状态等操作
- 评论管理：管理文章评论页面，显示用户已发布的全部文章，可以查看文章总评论数和粉丝评论数，可以对文章进行关闭评论等操作

- 素材管理：管理自媒体文章发布的图片，便于用户发布带有多张图片的文章

- 图文数据：自媒体人发布文章的数据：阅读数、评论数、收藏了、转发量，用户可以查看对应文章的阅读数据
- 粉丝画像：内容包括：粉丝性别分布、粉丝年龄分布、粉丝终端分布、粉丝喜欢分类分布

ADMIN功能大纲

- 用户管理：系统后台用来维护用户信息，可以对用户进行增删改查操作，对于违规用户可以进行冻结操
- 用户审核：管理员审核用户信息页面，用户审核分为身份审核和实名审核，身份审核是对用户的身份信息进行审核，包括但不限于工作信息、资质信息、经历信息等；实名认证是对用户实名身份进行认证

- 内容管理：管理员查询现有文章，并对文章进行新增、删除、修改、置顶等操作
- 内容审核：管理员审核自媒体人发布的内容，包括但不限于文章文字、图片、敏感信息等
- 频道管理：管理频道分类界面，可以新增频道，查看频道，新增或修改频道关联的标签

- 网站统计：统计内容包括：日活用户、访问量、新增用户、访问量趋势、热门搜索、用户地区分布等数据
- 内容统计：统计内容包括：文章采集量、发布量、阅读量、阅读时间、评论量、转发量、图片量等数据

- 权限管理：超级管理员对后台管理员账号进行新增或删除角色操作

### 1.2.2 技术架构

![1572946702002](img\1572946702002.png)

- Weex+Vue+WebSocket ：使用Weex跨平台开发工具，整合集成VUE框架，完成黑马头条移动端功能开发，并集成WebSocket实现即时消息（文章推荐、私信）的推送
- Vue+Echarts ： 自媒体系统使用Vue开发关键，集成Echarts图表框架，完成相关粉丝画像、数据分析等功能
- Vue+Echarts+WebSocket ： 管理系统也是使用Vue开发，集成Echarts，完成网站统计、内容统计等功能，集成WebSocket，实现系统看板实时数据自动化更新
- Spring-Cloud-Gateway : 微服务之前架设的网关服务，实现服务注册中的API请求路由，以及控制流速控制和熔断处理都是常用的架构手段，而这些功能Gateway天然支持
- PMD&P3C : 静态代码扫描工具，在项目中扫描项目代码，检查异常点、优化点、代码规范等，为开发团队提供规范统一，提升项目代码质量
- Junit : 在持续集成思想中，单元测试偏向自动化过程，项目通过Junit+Maven的集成实现这种过程
- 运用Spring Boot快速开发框架，构建项目工程；并结合Spring Cloud全家桶技术，实现后端个人中心、自媒体、管理中心等微服务。
- 运用WebMagic爬虫技术，完善系统内容自动化采集
- 运用Kafka完成内部系统消息通知；与客户端系统消息通知；以及实时数据计算
- 运用MyCat数据库中间件计算，对系统数据进行分开分表，提升系统数据层性能
- 运用Redis缓存技术，实现热数据的计算，NoSession等功能，提升系统性能指标
- 运用Zoookeeper技术，完成大数据节点之后的协调与管理，提升系统存储层高可用
- 使用Mysql存储用户数据，以保证上层数据查询的高性能
- 使用Mongo存储用户热数据，以保证用户热数据高扩展和高性能指标
- 使用FastDFS作为静态资源存储器，在其上实现热静态资源缓存、淘汰等功能
- 运用Habse技术，存储系统中的冷数据，保证系统数据的可靠性
- 运用ES搜索技术，对冷数据、文章数据建立索引，以保证冷数据、文章查询性能
- 运用Sqoop、Kettle等工具，实现大数据的离线入仓；或者数据备份到Hadoop
- 运用Spark+Hive进行离线数据分析，实现系统中各类统计报表
- 运用Spark Streaming + Hive+Kafka实现实时数据分析与应用；比如文章推荐
- 运用Neo4j知识图谱技术，分析数据关系，产出知识结果，并应用到上层业务中，以帮助用户、自媒体、运营效果/能力提升。比如粉丝等级计算
- 运用AI技术，来完成系统自动化功能，以提升效率及节省成本。比如实名认证自动化

# 2 黑马头条业务总结

## 2.1 文章发布业务

自媒体用户发布文章

![](img\文章发布.png)



![](img\文章列表.png)

## 2.2 文章采集业务

文章采集系统并不能直观的展示，是一个纯后台的项目，主要采集的是csdn中的文章信息，采集完成后需要保存到数据库，然后审核，最后才能展示给用户。

- 获取初始化url

![1574177606049](img\1574177606049.png)



- 定位初始化url中的个人空间地址

![1574177785941](img\1574177785941.png)

- 定位个人空间中的文章url

![1574177854803](img\1574177854803.png)

- 解析目标url中的具体内容

![1574177923495](img\1574177923495.png)

- 下载去重组件

![1574178020352](img\1574178020352.png)

- 保存数据到数据库  正向或逆向

![1574178105497](img\1574178105497.png)



## 2.3 自动审核文章业务

自动审核分为自媒体文章审核和爬虫文章审核两种方式

### 2.3.1 自媒体文章审核流程是什么？

![](img\自媒体文章审核.png)



具体代码

```java
@Service
@Log4j2
@SuppressWarnings("all")
public class ReviewMediaArticleServiceImpl implements ReviewMediaArticleService {

    @Autowired
    private WmNewsMapper wmNewsMapper;

    @Autowired
    private AliyunTextScanRequest aliyunTextScanRequest;

    @Autowired
    private AliyunImageScanRequest aliyunImageScanRequest;

    @Override
    public void autoReviewArticleByMedia(Integer newsId) {
        //根据文章id查询文章信息
        WmNews wmNews = wmNewsMapper.selectNewsDetailByPrimaryKey(newsId);

        if(wmNews!=null){
            //状态为4的时候，直接保存数据
            if(wmNews.getStatus()==4){
                reviewSuccessSaveAll(wmNews);
                return;
            }
            //审核通过后待发布文章，判断发布时间
            if(wmNews.getStatus()==8 && wmNews.getPublishTime()!=null && wmNews.getPublishTime().getTime()<new Date().getTime()){
                reviewSuccessSaveAll(wmNews);
                return;
            }
            //审核文章
            if(wmNews.getStatus()==1){
                //审核文章的标题与内容的匹配度
                String content = wmNews.getContent();
                String title = wmNews.getTitle();
                double degree = Compute.SimilarDegree(content, title);
                if(degree<=0){
                    //文章标题与内容匹配
                    updateWmNews(wmNews,(short)2,"文章标题与内容不匹配");
                    return;
                }
                //审核文本内容 阿里接口
                List<String> images = new ArrayList<>();
                StringBuilder sb = new StringBuilder();
                JSONArray jsonArray = JSON.parseArray(content);
                handlerTextAndImages(images,sb,jsonArray);

                try {
                    /*String response = aliyunTextScanRequest.textScanRequest(sb.toString());
                    if("review".equals(response)){//人工审核
                        updateWmNews(wmNews,(short)3,"需要人工审核");
                        return;
                    }
                    if("block".equals(response)){//审核失败
                        updateWmNews(wmNews,(short)2,"文本内容审核失败");
                        return;
                    }*/
                    //审核文章中的图片信息，阿里接口
                    /*String imagesResponse = aliyunImageScanRequest.imageScanRequest(images);
                    if(imagesResponse!=null){
                        if("review".equals(imagesResponse)){//人工审核
                            updateWmNews(wmNews,(short)3,"需要人工审核");
                            return;
                        }
                        if("block".equals(imagesResponse)){//审核失败
                            updateWmNews(wmNews,(short)2,"图片内容审核失败");
                            return;
                        }
                    }else{
                        updateWmNews(wmNews,(short)2,"图片审核出现问题");
                        return;
                    }*/

                } catch (Exception e) {
                    e.printStackTrace();
                }


                if(wmNews.getPublishTime()!=null){
                    if(wmNews.getPublishTime().getTime()>new Date().getTime()){
                        //修改wmnews的状态为8
                        updateWmNews(wmNews,(short)8,"待发布");
                    }else{
                        //立即发布
                        reviewSuccessSaveAll(wmNews);
                    }
                }else{
                    //立即发布
                    reviewSuccessSaveAll(wmNews);
                }
            }
        }

    }
}
```



### 2.3.2 爬虫文章审核与自媒体文章审核流程一样么？

![](img\爬虫文章审核.png)

具体代码：

```java
public class ReviewCrawlerArticleServiceImpl implements ReviewCrawlerArticleService {

    @Autowired
    private ClNewsMapper clNewsMapper;

    @Override
    public void autoReivewArticleByCrawler() throws Exception {
        ClNews clNews = new ClNews();
        clNews.setStatus((byte)1);
        List<ClNews> clNewsList = clNewsMapper.selectList(clNews);
        if(null != clNewsList && !clNewsList.isEmpty()){
            log.info("定时任务自动审核检索未审核数量：{}",clNewsList.size());
            for (ClNews news : clNewsList) {
                autoReivewArticleByCrawler(news);
            }
        }else{
            log.info("定时任务自动审核未检索出数据");
        }
    }

    @Override
    public void autoReivewArticleByCrawler(Integer clNewsId) throws Exception {
        ClNews clNews = new ClNews();
        clNews.setId(clNewsId);
        clNews.setStatus((byte)1);
        ClNews clNews1 = clNewsMapper.selectByIdAndStatus(clNews);
        if(null != clNews1){
            autoReivewArticleByCrawler(clNews1);
        }
    }

    @Autowired
    private AliyunTextScanRequest aliyunTextScanRequest;

    @Autowired
    private AliyunImageScanRequest aliyunImageScanRequest;

    @Autowired
    private AdChannelMapper adChannelMapper;

    @Override
    public void autoReivewArticleByCrawler(ClNews clNews) throws Exception {
        long currentTimeMillis = System.currentTimeMillis();
        log.info("开始自动审核流程");
        if(null != clNews){
            //审核内容和标题的匹配度
            String content = clNews.getUnCompressContent();
            String title = clNews.getTitle();
            if(content==null || title == null){
                updateClNews(clNews,"文章内容或标题为空");
                return ;
            }
            double degree = Compute.SimilarDegree(content, title);
            if(degree <= 0){
                updateClNews(clNews,"文章和标题不匹配");
                return ;
            }
            log.info("开始文本内容的审核");
            //审核图片和文本
            List<String> images = new ArrayList<>();
            StringBuilder sb = new StringBuilder();
            JSONArray jsonArray = JSON.parseArray(content);
            handlerTextAndImages(images,sb,jsonArray);
            //文本审核
            String response = aliyunTextScanRequest.textScanRequest(sb.toString());
            if(null == response || !response.equals("pass")){
                updateClNews(clNews,"文本内容审核失败");
                return ;
            }
            //审核图片
            String imageResponse = aliyunImageScanRequest.imageScanRequest(images);
            if(null == imageResponse || !imageResponse.equals("pass")){
                updateClNews(clNews,"图片内容审核失败");
                return ;
            }
            //保存数据 文章，文章配置，文章内容，作者，标签
            //频道的获取
            Integer channelId = clNews.getChannelId();
            String channelName = "";
            if(null != channelId){
                AdChannel adChannel = adChannelMapper.selectByPrimaryKey(channelId);
                if(null != adChannel){
                    channelName = adChannel.getName();
                }
            }
            //作者
            ApAuthor apAuthor = saveApAuthor(clNews);
            //文章
            ApArticle apArticle = saveApArticleByCrawler(images,channelId,channelName,apAuthor.getId(),clNews);
            //保存标签
            saveApArticleLabel(apArticle);
            //保存文章配置
            ApArticleConfig apArticleConfig = saveApArticleConfig(apArticle);
            //保存文章内容
            saveArArticleContent(content,apArticle);
            //创建索引
            try {
                createEsIndex(apArticle,clNews);
            }catch (Exception e){
                e.printStackTrace();
            }

            //更改状态 为 9
            updateClnewsSuccess(clNews);
        }
        log.info("审核流程结束，耗时:{}",System.currentTimeMillis()-currentTimeMillis);
    }
}
```



## 2.4 计算热点文章业务

热点文章数据，主要是根据什么维度来执行数据的计算

![](img\热文章计算流程.png)



计算的方式，主要是根据文章的点赞数量，转发数量，收藏数量，阅读数量等来统计文章的热度

统计行为：行为中包含了文章的以下动作，就是转发，收藏，阅读，点赞

热文章计算具体代码：

```java
public class ApHotArticleServiceImpl implements ApHotArticleService {

    @Autowired
    private ApArticleMapper apArticleMapper;

    @Autowired
    private ApBehaviorEntryMapper apBehaviorEntryMapper;

    @Autowired
    private ApHotArticlesMapper apHotArticlesMapper;

    @Autowired
    private KafkaSender kafkaSender;


    @Override
    public void computeHotArticle() {
        //查询前一天的新发布的文章列表
        String lastDay = DateTime.now().minusDays(1).toString("yyyy-MM-dd 00:00:00");
        List<ApArticle> articleList = apArticleMapper.loadLastArticleForHot(lastDay);
        //计算文章热度（阅读量，评论，点赞，收藏）
        List<ApHotArticles> hotArticlesList = computeHotArticle(articleList);
        //缓存频道首页到redis
        cacheTagRoRedis(articleList);
        //给每一个用户保存一份热点文章
        List<ApBehaviorEntry> entryList = apBehaviorEntryMapper.selectAllEntry();
        for (ApHotArticles hot : hotArticlesList) {
            //保存热点文章
            apHotArticlesMapper.insert(hot);
            //给每一个用户保存热点数据
            saveHotArticleForEntryList(hot,entryList);
            //缓存文章中的图片
            kafkaSender.sendHotArticleMessage(hot);
        }
    }

    /**
     * 给每一个用户保存一份热点数据
     * @param hot
     * @param entryList
     */
    private void saveHotArticleForEntryList(ApHotArticles hot, List<ApBehaviorEntry> entryList) {
        for (ApBehaviorEntry entry : entryList) {
            hot.setEntryId(entry.getId());
            apHotArticlesMapper.insert(hot);
        }
    }


    @Autowired
    private AdChannelMapper adChannelMapper;

    @Autowired
    private StringRedisTemplate redisTemplate;

    /**
     * 缓存频道首页数据到redis
     * @param articleList
     */
    private void cacheTagRoRedis(List<ApArticle> articleList) {
        List<AdChannel> channels = adChannelMapper.selectAll();
        List<ApArticle> temp = null;
        for (AdChannel channel : channels) {
            temp = articleList.stream().filter(p->p.getChannelId().equals(channel.getId())).collect(Collectors.toList());

            if(temp.size()>30){
                temp = temp.subList(0,30);
            }
            if(temp.size() == 0){
                redisTemplate.opsForValue().set(ArticleConstans.HOT_ARTICLE_FIRST_PAGE+channel.getId(),"");
                continue;
            }

            redisTemplate.opsForValue().set(ArticleConstans.HOT_ARTICLE_FIRST_PAGE+channel.getId(), JSON.toJSONString(temp));
        }
    }

    /**
     * 计算文章的分值
     *
     * @param apArticles
     * @return
     */
    public List<ApHotArticles> computeHotArticle(List<ApArticle> apArticles) {
        List<ApHotArticles> hotArticlesList = new ArrayList<>();
        ApHotArticles hot = null;
        for (ApArticle apArticle : apArticles) {
            hot = initHotBaseApArticle(apArticle);
            Integer score = computeScore(apArticle);
            hot.setScore(score);
            hotArticlesList.add(hot);
        }
        //通过排序取出前1000条数据
        hotArticlesList.sort(new Comparator<ApHotArticles>() {
            @Override
            public int compare(ApHotArticles o1, ApHotArticles o2) {
                return o1.getScore() < o2.getScore() ? 1 : -1;
            }
        });
        if(hotArticlesList.size() >1000){
            return hotArticlesList.subList(0,1000);
        }

        return hotArticlesList;
    }

    /**
     * 计算分值
     *
     * @param apArticle
     * @return
     */
    private Integer computeScore(ApArticle apArticle) {
        Integer score = 0;
        if (apArticle.getLikes() != null) {
            score += apArticle.getLikes();
        }
        if (apArticle.getCollection() != null) {
            score += apArticle.getCollection();
        }
        if (apArticle.getComment() != null) {
            score += apArticle.getComment();
        }
        if (apArticle.getViews() != null) {
            score += apArticle.getViews();
        }

        return score;
    }

    /**
     * 初始化热度文章信息
     *
     * @param apArticle
     * @return
     */
    private ApHotArticles initHotBaseApArticle(ApArticle apArticle) {
        ApHotArticles hot = new ApHotArticles();
        hot.setEntryId(0);
        hot.setTagId(apArticle.getChannelId());
        hot.setTagName(apArticle.getChannelName());
        hot.setScore(0);
        hot.setArticleId(apArticle.getId());
        hot.setProvinceId(apArticle.getProvinceId());
        hot.setCityId(apArticle.getCityId());
        hot.setCountyId(apArticle.getCountyId());
        hot.setIsRead(0);
        hot.setReleaseDate(apArticle.getPublishTime());
        hot.setCreatedTime(new Date());
        return hot;
    }
}
```

行为数据采集汇总：

```java
@Component
@Log4j2
public class HotArticleStreamHandler implements KafkaStreamListener<KStream<?, String>> {

    @Autowired
    private KafkaTopicConfig kafkaTopicConfig;

    @Autowired
    private ObjectMapper mapper;

    @Override
    public String listenerTopic() {
        return kafkaTopicConfig.getArticleUpdateBus();
    }

    @Override
    public String sendTopic() {
        return kafkaTopicConfig.getArticleIncrHandle();
    }

    @Override
    public KStream<?, String> getService(KStream<?, String> stream) {
        return stream.map((key, val) -> {
            UpdateArticleMessage value = format(val);
            System.out.println(value);
            //likes:1
            return new KeyValue<>(value.getData().getArticleId().toString(), value.getData().getType().name() + ":" + value.getData().getAdd());
        }).groupByKey().windowedBy(TimeWindows.of(10000)).aggregate(new Initializer<String>() {
            @Override
            public String apply() {
                return "COLLECTION:0,COMMENT:0,LIKES:0,VIEWS:0";
            }
        }, new Aggregator<Object, String, String>() {
            @Override
            public String apply(Object aggKey, String value, String aggValue) {
                //类似于  likes:1
                value = value.replace("UpdateArticle(", "").replace(")", "");
                String valAry[] = value.split(":");
                if ("null".equals(valAry[1])) {
                    return aggValue;
                }
                //"COLLECTION:0,COMMENT:0,LIKES:0,VIEWS:0";
//                String[] aggArr = aggValue.split(",");
                int col = 0, com = 0, lik = 0, vie = 0;
                if("LIKES".equalsIgnoreCase(valAry[0])){
                    lik+=Integer.valueOf(valAry[1]);
                }
                if("COLLECTION".equalsIgnoreCase(valAry[0])){
                    col+=Integer.valueOf(valAry[1]);
                }
                if("COMMENT".equalsIgnoreCase(valAry[0])){
                    com+=Integer.valueOf(valAry[1]);
                }
                if("VIEWS".equalsIgnoreCase(valAry[0])){
                    vie+=Integer.valueOf(valAry[1]);
                }
                /*for (int i = 0; i < aggArr.length; i++) {
                    String temp[] = aggArr[i].split(":");
                    switch (UpdateArticle.UpdateArticleType.valueOf(temp[0])) {
                        case COLLECTION:
                            col = Integer.valueOf(temp[1]);
                        case COMMENT:
                            com = Integer.valueOf(temp[1]);
                        case LIKES:
                            lik = Integer.valueOf(temp[1]);
                        case VIEWS:
                            vie = Integer.valueOf(temp[1]);
                    }

                }
                switch (UpdateArticle.UpdateArticleType.valueOf(valAry[0])) {
                    case COLLECTION:
                        col += Integer.valueOf(valAry[1]);
                    case COMMENT:
                        com += Integer.valueOf(valAry[1]);
                    case LIKES:
                        lik += Integer.valueOf(valAry[1]);
                    case VIEWS:
                        vie += Integer.valueOf(valAry[1]);
                }*/
                return String.format("COLLECTION:%d,COMMENT:%d,LIKES:%d,VIEWS:%d", col, com, lik, vie);
            }
        }, Materialized.as("count-article-num-miukoo-1")).toStream().map((key, value) -> {
            return new KeyValue<>(key.key().toString(), formatObj(key.key().toString(), value));
        });
    }

    private String formatObj(String articleId, String value) {
        String ret = "";
        ArticleVisitStreamMessage temp = new ArticleVisitStreamMessage();
        ArticleVisitStreamDto dto = new ArticleVisitStreamDto();
        String regEx = "COLLECTION:(\\d+),COMMENT:(\\d+),LIKES:(\\d+),VIEWS:(\\d+)";
        Pattern pat = Pattern.compile(regEx);
        Matcher mat = pat.matcher(value);
        if (mat.find()) {
            dto.setCollect(Long.valueOf(mat.group(1)));
            dto.setCommont(Long.valueOf(mat.group(2)));
            dto.setLike(Long.valueOf(mat.group(3)));
            dto.setView(Long.valueOf(mat.group(4)));
        } else {
            dto.setCollect(0);
            dto.setCommont(0);
            dto.setLike(0);
            dto.setView(0);
        }
        dto.setArticleId(Integer.valueOf(articleId));
        temp.setData(dto);
        try {
            ret = mapper.writeValueAsString(temp);
        } catch (JsonProcessingException e) {
            e.printStackTrace();
        }
        return ret;
    }

    private UpdateArticleMessage format(String val) {
        UpdateArticleMessage msg = null;
        try {
            msg = mapper.readValue(val, UpdateArticleMessage.class);
        } catch (IOException e) {
            e.printStackTrace();
        }
        return msg;
    }
}
```



## 2.5 数据迁移业务

数据迁移是为了缓解系统的中的压力，让项目支撑更多的数据存储

![1573474478661](img\1573474478661.png)

迁移到hbase具体代码：

```java
public class ArticleQuantityServiceImpl implements ArticleQuantityService {

    @Autowired
    private ApArticleConfigService apArticleConfigService;

    @Autowired
    private ApArticleContentService apArticleContentService;

    @Autowired
    private ApArticleService apArticleService;

    @Autowired
    private ApAuthorService apAuthorService;

    @Autowired
    private HBaseStorageClient storageClient;

    @Override
    public List<ArticleQuantity> getArticleQuantityList() {
        log.info("生成ArticleQuantity列表");
        List<ApArticle> unSyncArticleList = apArticleService.getUnSyncArticleList();
        //获取文章id列表
        List<String> apArticleIdList = unSyncArticleList.stream().map(apArticle -> String.valueOf(apArticle.getId())).collect(Collectors.toList());
        //获取文章中的作者id列表
        List<Integer> apAuthorIdList = unSyncArticleList.stream().map(apArticle -> apArticle.getAuthorId() == null ? null : apArticle.getAuthorId().intValue()).filter(x -> x != null).collect(Collectors.toList());
        //根据文章id列表查询文章配置列表和文章内容列表
        List<ApArticleConfig> apArticleConfigList = apArticleConfigService.queryByArticleIds(apArticleIdList);
        List<ApArticleContent> apArticleContentList = apArticleContentService.queryByArticleIds(apArticleIdList);
        //根据作者id列表查询作者列表
        List<ApAuthor> apAuthorList = apAuthorService.queryByIds(apAuthorIdList);
        //综合ArticleQuantity  --- list
        List<ArticleQuantity> articleQuantityList = unSyncArticleList.stream().map(apArticle -> {
            return new ArticleQuantity(){{
                setApArticle(apArticle);
                //根据文章的id过滤出符合要求的内容对象
                List<ApArticleContent> apArticleContents = apArticleContentList.stream().filter(x -> x.getArticleId().equals(apArticle.getId())).collect(Collectors.toList());
                if(null != apArticleContents && !apArticleContents.isEmpty()){
                    setApArticleContent(apArticleContents.get(0));
                }
                List<ApArticleConfig> apArticleConfigs = apArticleConfigList.stream().filter(x -> x.getArticleId().equals(apArticle.getId())).collect(Collectors.toList());
                if(null != apArticleConfigs && !apArticleConfigs.isEmpty()){
                    setApArticleConfig(apArticleConfigs.get(0));
                }
                List<ApAuthor> apAuthors = apAuthorList.stream().filter(x -> x.getId().equals(apArticle.getAuthorId().intValue())).collect(Collectors.toList());
                if(null != apAuthors && !apAuthors.isEmpty()){
                    setApAuthor(apAuthors.get(0));
                }
                //设置回调方法，用户方法的回调，用于修改同步的状态，查询hbase，成功后同步状态修改
                setHBaseInvok(new ArticleHBaseInvok(apArticle,(x)->apArticleService.updateSyncStatus(x)));
            }};
        }).collect(Collectors.toList());

        if(null != articleQuantityList && !articleQuantityList.isEmpty()){
            log.info("生成articleQuantity列表完成，size:{}",articleQuantityList.size());
        }else {
            log.info("生成articleQuantity列表完成，size:{}",0);
        }
        return articleQuantityList;
    }

    @Override
    public ArticleQuantity getArticleQuantityByArticleId(Long id) {
        if(null == id){
            return  null;
        }
        ArticleQuantity articleQuantity = null;
        ApArticle apArticle = apArticleService.getById(id);
        if(null != apArticle){
            articleQuantity = new ArticleQuantity();
            articleQuantity.setApArticle(apArticle);
            ApArticleContent apArticleContent = apArticleContentService.getByArticleId(apArticle.getId());
            articleQuantity.setApArticleContent(apArticleContent);
            ApArticleConfig apArticleConfig = apArticleConfigService.getByArticleId(apArticle.getId());
            articleQuantity.setApArticleConfig(apArticleConfig);
            ApAuthor apAuthor = apAuthorService.getById(apArticle.getAuthorId());
            articleQuantity.setApAuthor(apAuthor);
        }
        return articleQuantity;
    }

    @Override
    public ArticleQuantity getArticleQuantityByArticleidForHbase(Long id) {
        if(null == id){
            return null;
        }
        ArticleQuantity articleQuantity = null;
        List<Class> typeList = Arrays.asList(ApArticle.class,ApArticleConfig.class,ApArticleContent.class,ApAuthor.class);
        List<Object> objectList = storageClient.getStorageDataEntityList(HBaseConstants.APARTICLE_QUANTITY_TABLE_NAME, DataConvertUtils.toString(id), typeList);
        if(null != objectList && !objectList.isEmpty()){
            articleQuantity = new ArticleQuantity();
            for (Object value : objectList) {
                if(value instanceof ApArticle){
                    articleQuantity.setApArticle((ApArticle) value);
                }else if(value instanceof  ApArticleContent){
                    articleQuantity.setApArticleContent((ApArticleContent) value);
                }else if(value instanceof ApArticleConfig){
                    articleQuantity.setApArticleConfig((ApArticleConfig) value);
                }else if(value instanceof ApAuthor){
                    articleQuantity.setApAuthor((ApAuthor) value);
                }
            }
        }
        return articleQuantity;
    }

    @Override
    public void dbToHbase() {
        long currentTimeMillis = System.currentTimeMillis();
        List<ArticleQuantity> articleQuantityList = getArticleQuantityList();
        if(null != articleQuantityList&& !articleQuantityList.isEmpty()){
            log.info("开始进行定时数据库到hbase同步，帅选出未同步的数据量:{}",articleQuantityList.size());
            List<HBaseStorage> hBaseStorageList = articleQuantityList.stream().map(ArticleQuantity::getHbaseStorage).collect(Collectors.toList());
            storageClient.addHBaseStorage(HBaseConstants.APARTICLE_QUANTITY_TABLE_NAME,hBaseStorageList);
        }else {
            log.info("定时数据库到hbase中，没有找到数据");
        }
        log.info("定时数据库到hbase同步结束,耗时:{}",System.currentTimeMillis()-currentTimeMillis);

    }

    @Override
    public void dbToHbase(Integer articleId) {
        long currentTimeMillis = System.currentTimeMillis();
        log.info("开始进行异步数据库到hbase同步，articleId:{}",articleId);
        if(null != articleId){
            ArticleQuantity articleQuantity = getArticleQuantityByArticleId(articleId.longValue());
            if(null!=articleQuantity){
                HBaseStorage hbaseStorage = articleQuantity.getHbaseStorage();
                storageClient.addHBaseStorage(HBaseConstants.APARTICLE_QUANTITY_TABLE_NAME,hbaseStorage);
            }
        }
        log.info("异步数据库到hbase同步完成，articleId:{},耗时:{}",articleId,System.currentTimeMillis()-currentTimeMillis);
    }
}
```

热点数据迁移具体代码：

```java
public class ApHotArticleServiceImpl implements ApHotArticleService {

    @Autowired
    private ApHotArticlesMapper apHotArticlesMapper;

    @Autowired
    private MongoTemplate mongoTemplate;

    @Autowired
    private HBaseStorageClient hBaseStorageClient;

    @Autowired
    private ArticleQuantityService articleQuantityService;

    @Override
    public List<ApHotArticles> selectList(ApHotArticles apHotArticles) {
        return apHotArticlesMapper.selectList(apHotArticles);
    }

    @Override
    public void insert(ApHotArticles apHotArticles) {
        apHotArticlesMapper.insert(apHotArticles);
    }

    @Override
    public void hotApArticleSync(Integer articleId) {
        //根据文章id查询数据
        log.info("开始热数据同步，文章id:{}", articleId);
        ArticleQuantity articleQuantity = getHotArticleQuantity(articleId);
        if (articleQuantity != null) {
            //同步热点数据到数据库
            hotApArticleToDBSync(articleQuantity);
            //同步数据到mongo
            hotApArticleToMongoSync(articleQuantity);
            log.info("热数据同步完成，articleId:{}", articleId);
        } else {
            log.info("找不到对应的数据，articleId:{}", articleId);
        }

    }

    /**
     * 同步到数据库
     *
     * @param articleQuantity
     */
    private void hotApArticleToDBSync(ArticleQuantity articleQuantity) {
        //新通过文章id查询，数据库是否存在
        Integer apArticleId = articleQuantity.getApArticleId();
        log.info("开始将热数据同步到mysql,apArticleId:{}", apArticleId);
        if (null == apArticleId) {
            log.info("apArticleId不存在，无法进行同步");
            return;
        }
        ApHotArticles apHotArticles = new ApHotArticles();
        apHotArticles.setArticleId(apArticleId);
        List<ApHotArticles> hotArticlesList = apHotArticlesMapper.selectList(apHotArticles);
        if (null != hotArticlesList && !hotArticlesList.isEmpty()) {
            //存在  则不需要保存
            log.info("mysql数据已同步，不需要再次同步，apArticleId:{}", apArticleId);
        } else {
            //不存在，新增热点数据
            ApHotArticles temp = articleQuantity.getApHotArticles();
            apHotArticlesMapper.insert(temp);
        }


    }

    /**
     * 同步到mongo
     *
     * @param articleQuantity
     */
    private void hotApArticleToMongoSync(ArticleQuantity articleQuantity) {
        Integer apArticleId = articleQuantity.getApArticleId();
        log.info("开始将热数据同步到mongo,apArticleId:{}", apArticleId);
        if (null == apArticleId) {
            log.info("apArticleId不存在，无法进行同步");
            return;
        }
        String rowKey = DataConvertUtils.toString(apArticleId);
        MongoStorageEntity mongoStorageEntity = mongoTemplate.findById(rowKey, MongoStorageEntity.class);
        if (null != mongoStorageEntity) {
            log.info("Mongo数据已同步，不需要再次同步，apArticleId:{}", apArticleId);
        } else {
            List<StorageData> storageDataList = articleQuantity.getStorageDataList();
            if(null != storageDataList && !storageDataList.isEmpty()){
                mongoStorageEntity = new MongoStorageEntity();
                mongoStorageEntity.setRowKey(rowKey);
                mongoStorageEntity.setDataList(storageDataList);
                mongoTemplate.insert(mongoStorageEntity);
            }
        }
        log.info("将数据同步到mongo完成，apArticleId:{}",apArticleId);
    }

    /**
     * 获取热数据的articleQuantity
     *
     * @param articleId
     * @return
     */
    private ArticleQuantity getHotArticleQuantity(Integer articleId) {
        ArticleQuantity articleQuantity = articleQuantityService.getArticleQuantityByArticleId(articleId.longValue());
        if (null == articleQuantity) {
            articleQuantity = articleQuantityService.getArticleQuantityByArticleidForHbase(articleId.longValue());
        }
        return articleQuantity;
    }

    @Override
    public void deleteById(Integer id) {
        apHotArticlesMapper.deleteById(id);
    }

    @Override
    public List<ApHotArticles> selectExpireMonth() {
        return apHotArticlesMapper.selectExpireMonth();
    }

    @Override
    public void deleteHotData(ApHotArticles apHotArticles) {
        deleteById(apHotArticles.getId());
        String rowKey = DataConvertUtils.toString(apHotArticles.getArticleId());
        hBaseStorageClient.gethBaseClent().deleteRow(HBaseConstants.APARTICLE_QUANTITY_TABLE_NAME,rowKey);
        MongoStorageEntity storageEntity = mongoTemplate.findById(rowKey, MongoStorageEntity.class);
        if(null!=storageEntity){
            mongoTemplate.remove(storageEntity);
        }
    }
}
```





# 3 黑马头条技术总结

## 3.1 Mycat

### 3.1.1 Mycat是什么？

 MyCat是一个开源的分布式数据库中间件系统，是一个实现了MySQL协议的中间件服务，其主要功能有：

- 用于MySQL读写分离和与数据切分的高可用中间件
- 模拟为MySQLServer的超级数据库代理 
- 能平滑扩展支持1000亿大表的分布式数据库系统 
- 可管控多种关系数据库的数据库路由器 

### 3.1.2 Mycat有哪些特点？

- 遵守Mysql原生协议，跨语言，跨数据库的通用中间件代理。

- 基于心跳的自动故障切换，支持读写分离，支持MySQL一双主多从，以及一主多从

- 有效管理数据源连接，基于数据分库，而不是分表的模式。

- 基于Nio实现，有效管理线程，高并发问题。

- 支持数据的多片自动路由与聚合，支持sum,count,max等常用的聚合函数。

- 支持2表join，甚至基于caltlet的多表join。

- 支持通过全局表，ER关系的分片策略，实现了高效的多表join查询。

- 支持多租户方案。-支持分布式事务（弱xa）

- 支持全局序列号，解决分布式下的主键生成问题。

- 分片规则丰富，插件化开发，易于扩展。

- 强大的web，命令行监控。

- 支持前端作为mysq通用代理，后端JDBC方式支持Oracle、DB2、SQL Server、mongodb、巨杉。

- 集群基于ZooKeeper管理，在线升级，扩容，智能优化，大数据处理。

### 3.1.3 Mycat 架构？

 ![img](img\1.jpg) 

### 3.1.4 Mycat核心概念？

- 数据源：存储数据的数据库服务器，区分为读数据源和写数据源；通过dataHost定义
- 数据点：数据源存储数据的数据库，通过dataNode定义
- 逻辑库：聚合多个数据节点上的多个数据库，通过schema定义
- 逻辑表：分库分表的逻辑定义视图，类似VIEW，集合查询数据，实际不存储数据，通过table定义
- 分片函数：定义分表数据计算的函数，主要指定分多少表，通过function定义
- 分片规则：每个表数据分片存储的规则，主要指定分片字段和分片函数，通过tableRule定义

### 3.1.5 为什么要用Mycat?

​		基本上现代系统都具有数据量大的特点，而关系型数据库单表最优性能支持1000W级别，所以在实际项目开发过程中对于超出单表容量的数据基本采用分表来进行处理。直接在程序中进行分表实现的方式再后期很难维护，因此提出了专门的分库分表中间系统，为程序提供透明的服务，而分库分表在Mycat中自动处理。

​		在黑马头条项目中，需求中设计的用户容量为1亿，所以需要进行分库分表才能保证单表的性能。

### 3.1.6 如何设计分库分表？

#### 3.1.6.1 分库分表设计

分库分表是需要设计的，设计的目标是解决以下几个问题：

- 是否需要分库分表？
- 表数据是否永久存储？
- 表日增数据量是多少？
- 现有资源能支持多少表？
- 后期表是否需要水平扩展？
- 按照表的那个字段分更合理？
- 分表数据是否需要读写负载？

黑马头条便是带着以上问题，一张一张表的设计，并汇总成以下格式的表格，便于开发与后期维护：

![1573204075501](img\mycat.png)

#### 3.1.6.2 分库分表函数设计

通过分库分表的设计与总结，会发现Mycat支持的分表函数库可能不能满足我们的实际需要，此时就需要设计自己的分库分表函数。

在黑马头条项目中，设计了HeiMaBurstRuleAlgorithm函数算法，支持两个字段的复合分库分表。第一个字段是数据的自增ID，用于计算数据存储的数据库分组（一组包括独立的分库分表结构），第二个字段是分表字段，用于计算数据在组内存储表位置的计算。改算法的优点是水平拓展良好，当一组节点的数据存储满时，再扩展一组节点即可，而不用做程序改动。

```java
/**
* 需要继承AbstractPartitionAlgorithm
* 需要实现RuleAlgorithm
*/
public class HeiMaBurstRuleAlgorithm extends AbstractPartitionAlgorithm implements RuleAlgorithm {
    // 单值路由计算调用方法
    public Integer calculate(String columnValue){}
    // 范围值路由计算调用方法
    public Integer[] calculateRange(String beginValue, String endValue){}
}
```

## 3.2 weex

### 3.2.1 Weex是什么？

 Weex 致力于使开发者能基于通用跨平台的 Web 开发语言和开发经验，来构建 Android、iOS 和 Web 应用。简单来说，在集成了 WeexSDK 之后，你可以使用 JavaScript 语言和前端开发经验来开发移动应用。 而且是写一次代码可在Android、iOS 和 Web 三端运行。

### 3.2.2 Weex架构原理？

Weex的原理很简单，提供标准的V-DOM，上层的DSL通过transformer转换成标准的V-DOM，然后分发在不同终端上通过Renderer加载解析，并输出原生的UI界面。其中开发者者只需要关注最上层的DSL层即可。

![1573206500483](img\weex.png)

### 3.2.3 Weex开发与网页开发的区别？

- Weex网络请求，需要使用内置模块，或者自定义的模块。不能直接使用Ajax
- Weex的默认宽度是750px，渲染时会自动以此标准计算屏幕尺寸，达到自适应的目的
- Weex样式单位只支持wx和px，wx是不随屏幕大小变更的大小的单位
- Weex页面布局只支持Flexbox

## 3.3 数据传输序列化

### 3.3.1 什么是数据序列化？

 序列化 (Serialization)是将对象的状态信息转换为可以存储或传输的形式的过程。

### 3.3.2 为什么要序列化？

- 减小内存空间和网络传输的带宽

- 分布式的可扩展性

- 通用性，接口可共用。

### 3.3.3 常见的数据序列化有哪些？

- Java对象序列化，需要实现 Serializable 接口
- Json序列化，常用于http协议请求
- Xml序列化，常用于http协议请求
- Hessian 序列化， dubbo协议默认使用的数据序列化形式

## 3.4 项目多环境配置

### 3.4.1 为什么要多环境配置？

多环境配置便于开发、测试、生产环境的配置属性分开管理，同时便于三方协同。

### 3.4.2 如何使用多环境配置？

在每一个微服务的工程中的根目录下创建三个文件，方便各个环境的切换

（1）maven_dev.properties  

​	定义开发环境的配置

（2）maven_prod.properties  

​	定义生产环境的配置

（3）maven_test.properties   

​	定义测试环境的配置，开发阶段使用这个测试环境

**默认加载的环境为test，在打包的过程中也可以指定参数打包  package -P test/prod/dev**

具体配置，请查看父工程下的maven插件的profiles配置

```xml
<profiles>
    <profile>
        <id>dev</id>
        <build>
            <filters>
                <filter>maven_dev.properties</filter>
            </filters>
        </build>
    </profile>
    <profile>
        <id>test</id>
        <activation>
            <activeByDefault>true</activeByDefault>
        </activation>
        <build>
            <filters>
                <filter>maven_test.properties</filter>
            </filters>
        </build>
    </profile>
    <profile>
        <id>prod</id>
        <build>
            <filters>
                <filter>maven_prod.properties</filter>
            </filters>
        </build>
    </profile>
</profiles>
```

## 3.5 通用的mysql封装和事务封装

### 3.5.1 数据源封装步骤是什么？

- 导入依赖驱动和ORM包
- 添加配置属性（包含连接信息、密码、用户等）
- 创建数据库连接池（Spring默认Hikari连接池）
- 集成ORM配置（SqlSessionFactoryBean）
- 配置事务管理器

### 3.5.2 如何使用JavaConfig方式配置事务管理器？

```java
@Setter
@Getter
@Aspect
@EnableAspectJAutoProxy
@EnableTransactionManagement
@Configuration
@ConfigurationProperties(prefix="mysql.core")
@PropertySource("classpath:mysql-core-jdbc.properties")
public class TransactionConfig {

    String txScanPackage;

    /**
     * 初始化事务管理器
     * @param dataSource
     * @return
     */
    @Bean
    public DataSourceTransactionManager mysqlCoreDataSourceTransactionManager(@Qualifier("mysqlCoreDataSource") DataSource dataSource){
        DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager();
        dataSourceTransactionManager.setDataSource(dataSource);
        return dataSourceTransactionManager;
    }

    /**
     * 设置事务拦截器
     * @param dataSourceTransactionManager
     * @return
     */
    @Bean
    public TransactionInterceptor mysqlCoreDataSourceTxAdvice(@Qualifier("mysqlCoreDataSourceTransactionManager") DataSourceTransactionManager dataSourceTransactionManager) {
        // 默认事务
        DefaultTransactionAttribute defAttr = new DefaultTransactionAttribute(TransactionDefinition.PROPAGATION_REQUIRED);
        // 查询只读事务
        DefaultTransactionAttribute queryAttr = new DefaultTransactionAttribute(TransactionDefinition.PROPAGATION_REQUIRED);
        queryAttr.setReadOnly(true);
        // 设置拦截的方法
        NameMatchTransactionAttributeSource source = new NameMatchTransactionAttributeSource();
        source.addTransactionalMethod("save*", defAttr);
        source.addTransactionalMethod("insert*", defAttr);
        source.addTransactionalMethod("delete*", defAttr);
        source.addTransactionalMethod("update*", defAttr);
        source.addTransactionalMethod("exec*", defAttr);
        source.addTransactionalMethod("set*", defAttr);
        source.addTransactionalMethod("add*", defAttr);
        source.addTransactionalMethod("get*", queryAttr);
        source.addTransactionalMethod("query*", queryAttr);
        source.addTransactionalMethod("find*", queryAttr);
        source.addTransactionalMethod("list*", queryAttr);
        source.addTransactionalMethod("count*", queryAttr);
        source.addTransactionalMethod("is*", queryAttr);

        return new TransactionInterceptor(dataSourceTransactionManager, source);
    }

    @Bean
    public Advisor txAdviceAdvisor(@Qualifier("mysqlCoreDataSourceTxAdvice") TransactionInterceptor mysqlCoreDataSourceTxAdvice) {
        AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut();
        pointcut.setExpression(txScanPackage);
        return new DefaultPointcutAdvisor(pointcut, mysqlCoreDataSourceTxAdvice);
    }

}
```



## 3.6 分布式主键(zk实现)

### 3.6.1 分布式主键是什么？

分布式主键是一个全局唯一的主键生成器，需要在分布式系统中保障高并发性能生成和并发一致。场景的分布式主键生成策略方式有：

- 雪花算法
- Redis分布式自增器
- ZK原子操作类（黑马头条就是采用此方式）

### 3.6.2 ZK如何实现分布式主键？

在ZK中支持分布式原子操作，相关类是DistributedAtomicLong，及实现代码如下：

```java
public class ZkSequence {

    RetryPolicy retryPolicy = new ExponentialBackoffRetry(500, 3);

    DistributedAtomicLong distAtomicLong;

    public ZkSequence(String sequenceName, CuratorFramework client){
        distAtomicLong = new DistributedAtomicLong(client,sequenceName,retryPolicy);
    }

    /**
     * 生成序列
     * @return
     * @throws Exception
     */
    public Long sequence() throws Exception{
        AtomicValue<Long> sequence = this.distAtomicLong.increment();
        if(sequence.succeeded()){
            return sequence.postValue();
        }else{
            return null;
        }
    }

}
```

## 3.7 通用后台封装

### 3.7.1 为什么后台功能要封装？

封装可以便于维护，更重要的是能提高开发效率。

### 3.7.2 通过后台封装的实现思路是什么？

【持久层】

-   通过动态SQL或者SQL拼接方式，实现通用的持久层

【业务层】

-   定义操作权限类，控制数据表的基本操作权限（是否允许CRUD），每次操作之前需要检查此表

-   每次操作之前检查当前登录人员是否有此功能操作权限（此步要与RBAC进行基础，此例不做实现）

-   参数校验（比如更新时需要查询条件）

-   参数合法性校验（放置SQL注入）

-   前置处理逻辑调用（前置结果可影响程序的继续执行，此例作为练习内容，在此步实现）

-   数据持久化操作

-   后置处理逻辑调用（后置结果可影响返回结果）

-   返回结果

## 3.8 kafka

### 3.8.1 Kafka是什么？

 Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据 。

### 3.8.2 Kafka的封装思路是？

![1569418527993](img\1569418527993.png)

- KafkaProducerConfig自动配置Kafka消费者
- KafkaConsumerConfig自动配置Kafka消费者
- RetryErrorHandler实现消费者处理消息失败后重新发送到消息队列
- KafkaMessage实现对发送的消息包装，提供重试次数、分类等信息
- KafkaSender实现消息的统一发送入口功能
- KafkaTopicConfig自动装载topic名称信息
- KafkaListener提供自动注册消息消费监听接口类
- KafkaListenerFactory提供启动时自动注册实现了KafkaListener的消息消费者

### 3.8.3 Kafka Stream如何封装使用？

实现思路：

- 封装消息监听器接口（支持KStream和KTable）
- 项目启动，扫描实现了监听器接口的类注册到Bean池中
- 项目启动后，从Bean池扫描实现了KafkaStreamListener的Bean并注册为对象流处理器

以上思路实现的核心代码如下：

```java
/**
 * KafkaStream自动处理包装类
 */
public class KafkaStreamProcessor {

    // 流构建器
    StreamsBuilder streamsBuilder;
    private String type;
    KafkaStreamListener listener;

    public KafkaStreamProcessor(StreamsBuilder streamsBuilder,KafkaStreamListener kafkaStreamListener){
        this.streamsBuilder = streamsBuilder;
        this.listener = kafkaStreamListener;
        this.parseType();
        Assert.notNull(this.type,"Kafka Stream 监听器只支持kstream、ktable,当前类型是"+this.type);
    }

    /**
     * 通过泛型类型自动注册对应类型的流处理器对象
     * 支持KStream、KTable
     * @return
     */
    public Object doAction(){
        if("kstream".equals(this.type)) {
            KStream<?, ?> stream = streamsBuilder.stream(listener.listenerTopic(), Consumed.with(Topology.AutoOffsetReset.LATEST));
            stream=(KStream)listener.getService(stream);
            stream.to(listener.sendTopic());
            return stream;
        }else{
            KTable<?, ?> table = streamsBuilder.table(listener.listenerTopic(), Consumed.with(Topology.AutoOffsetReset.LATEST));
            table = (KTable)listener.getService(table);
            table.toStream().to(listener.sendTopic());
            return table;
        }
    }

    /**
     * 解析传入listener类的泛型类
     */
    private void parseType(){
        Type[] types = listener.getClass().getGenericInterfaces();
        if(types!=null){
            for (int i = 0; i < types.length; i++) {
                if( types[i] instanceof ParameterizedType){
                    ParameterizedType t = (ParameterizedType)types[i];
                    String name = t.getActualTypeArguments()[0].getTypeName().toLowerCase();
                    if(name.contains("org.apache.kafka.streams.kstream.kstream")||name.contains("org.apache.kafka.streams.kstream.ktable")){
                        this.type = name.substring(0,name.indexOf('<')).replace("org.apache.kafka.streams.kstream.","").trim();
                        break;
                    }
                }
            }
        }
    }

}
```



## 3.9 爬虫

### 3.9.1 文章采集使用的是什么技术？

文章采集使用的框架是webmagic

![1573198117342](E:/heima-leadnews/%E9%BB%91%E9%A9%AC%E5%A4%B4%E6%9D%A1%E8%AF%BE%E4%BB%B6/%E9%BB%91%E9%A9%AC%E5%A4%B4%E6%9D%A1-%E5%B0%B1%E4%B8%9A%E6%8C%87%E5%AF%BC/%E7%AC%94%E8%AE%B0/img/1573198117342.png)

- Downloader

  Downloader负责从互联网上下载页面，以便后续处理。WebMagic默认使用了[Apache HttpClient](http://hc.apache.org/index.html)作为下载工具。

- PageProcessor

  PageProcessor负责解析页面，抽取有用信息，以及发现新的链接。WebMagic使用[Jsoup](http://jsoup.org/)作为HTML解析工具，并基于其开发了解析XPath的工具[Xsoup](https://github.com/code4craft/xsoup)。

  在这四个组件中，PageProcessor对于每个站点每个页面都不一样，是需要使用者定制的部分。

- Scheduler

  Scheduler负责管理待抓取的URL，以及一些去重的工作。WebMagic默认提供了JDK的内存队列来管理URL，并用集合来进行去重。也支持使用Redis进行分布式管理。

  除非项目有一些特殊的分布式需求，否则无需自己定制Scheduler。

- Pipeline

  Pipeline负责抽取结果的处理，包括计算、持久化到文件、数据库等。WebMagic默认提供了“输出到控制台”和“保存到文件”两种结果处理方案。

  Pipeline定义了结果保存的方式，如果你要保存到指定数据库，则需要编写对应的Pipeline。对于一类需求一般只需编写一个Pipeline。

  更多内容可以查看官网文档 http://webmagic.io/docs/zh/

### 3.9.2 如何解析html标签？

Xpath的定位

xpath的定位主要由路径定位、标签定位、轴定位组合构成，外加筛选功能进行辅助，几乎可以定位到任意元素

- 标签定位

- 路径定位

- 轴定位

- 筛选

  ​	属性筛选、序号筛选

### 3.9.3 ip代理池的管理？

当我们对某些网站进行爬去的时候，我们经常会换IP来避免爬虫程序被封锁。其实也是一个比较简单的操作，目前网络上有很多IP代理商，例如西刺，芝麻，犀牛等等。这些代理商一般都会提供透明代理，匿名代理，高匿代理。

- 代理IP类型：

  代理IP一共可以分成4种类型。前面提到过的透明代理IP，匿名代理IP，高匿名代理IP，还有一种就是混淆代理IP。最基础的安全程度来说呢，他们的排列顺序应该是这个样子的高匿 > 混淆 > 匿名 > 透明。

- 管理本地ip代理池

  本地库中保存一份代理池，这些ip会定时去做检查，如果可以使用则留下来，如果不能用则标识删除，也会定时要免费的ip代理商网站上去爬取一些新的ip,来更新ip代理池，保证项目正常采集文章。

### 3.9.4 文章采集遇到过什么问题？

因为csdn的cookie通过js来生成的，需要浏览器才能得到Cookie,我们采用Selenium 调用chrome浏览器来获取必须要的Cookie

Selenium 是一个用于 Web 应用程序测试的工具。它的优点在于，浏览器能打开的页面，使用 selenium 就一定能获取到。但 selenium 也有其局限性，相对于脚本方式，selenium 获取内容的效率不高。

## 3.10 项目优化解决方案

### 3.10.1 项目优化包括哪些？

- 易用性：项目功能能够简便实用，可以利用一些自动化、提升UE等方式实施
- 安全性：项目安全很重要，但是实现安全需要牺牲部分性能，需要找到平衡
- 高性能：性能是每个系统的追求核标准，可以通过缓存、算法、代码优化
- 高可用：仅好用不够，还需要7x24小时都好用才行，可通过运维和架构优化
- 可维护：越来越庞大的项目，代码和数据越来越多，设计合理和实现优雅是现代系统的追求目标

## 3.11 Hbase

### 3.11.1 Hbase在数据迁移中的应用场景？

- 将mysql数据库中的全量数据定时读取出来，将多个对象打包成一个对象，保存到HBASE中，保存成功后更新数据库中的状态改为已同步，下一次就不会同步该条数据了。
- 使用KAFKA监听热点数据计算结果，接收到热点数据信息后，从HBASE得到打包的数据，并将数据进行拆分，将关系数据保存到mysql中，将具体数据保存到mongodb中。
- 因为热点数据会失效，定期清除mysql和mongodb中的过期数据

### 3.11.2 Hbase的使用注意事项？

- HBASE数据主要靠rowKey进行查询的，rowKey设计就用mysql中的主键ID作为rowKey，查询的时候直接根据Rowkey获取数据
- 因为需要同步到HBASE的数据是多个数据表的数据，一条数据由多个对象组成，存储的时候使用列族区分不同的对象，里面存储不同的字段。

## 3.12 Quartz

### 3.12.1 什么是Quartz?

 Quartz 是一个完全由 Java 编写的开源作业调度框架，为在 Java 应用程序中进行作业调度提供了简单却强大的机制 ，并且支持集群任务。

### 3.12.2 Quzrtz如何封装简化使用？

实现的思路：

- 提供一个默认的调度任务在项目启动时运行一次
- 默认调度任务从Bean池中扫描继承了AbstractJob类的Bean
- 把相关Bean注册成Job，如果已存在则删除后重新增加

以上思路实现的核心代码如下：

```java
@Data
@Log4j2
@DisallowConcurrentExecution
@PersistJobDataAfterExecution
@Transactional
public class QuartzScanJob extends QuartzJobBean {

    @Value("branch-${info.git.branch?:default}")
    String branch;
    @Value("${spring.quartz.group-prefix}")
    String groupPrefix;
    @Autowired
    SchedulerFactoryBean schedulerFactoryBean;
    @Autowired
    DefaultListableBeanFactory defaultListableBeanFactory;
    @Autowired
    private Scheduler scheduler;
    // 描述器后缀
    private static final String DETAIL_SUFFIX = "AutoJobDetail";
    // 触发器后缀
    private static final String TRIGGER_SUFFIX = "AutoTrigger";

    @Override
    protected void executeInternal(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String temp = (String)jobExecutionContext.getJobDetail().getJobDataMap().get("branch");
        log.info("当前程序环境是[{}]，变量环境是：[{}]",getBranch(),temp);
        if(!branch.equalsIgnoreCase(temp)) {
            Map<String, AbstractJob> abs = defaultListableBeanFactory.getBeansOfType(AbstractJob.class);
            if(abs!=null){
                this.clearGroupJobAndTrigger(abs);
                for (String key : abs.keySet()) {
                    AbstractJob job = abs.get(key);
                    if(job.isAutoOverwrite()) {
                        String detailBeanName = key + DETAIL_SUFFIX;
                        createJobDetail(key, detailBeanName, job);
                        this.createdTrigger((JobDetail) defaultListableBeanFactory.getBean(detailBeanName), key, job);
                    }
                }
            }
            jobExecutionContext.getJobDetail().getJobDataMap().put("branch",getBranch());
        }else{
            log.info("============= skip auto init jobs");
        }
    }

    /**
     * 清理掉当前分组的JOB和触发器信息
     * @param abs
     */
    private void clearGroupJobAndTrigger(Map<String, AbstractJob> abs){
        try {
            Set<JobKey> jobKeys = scheduler.getJobKeys(GroupMatcher.groupStartsWith(getGroupPrefix()));
            for (JobKey jobKey : jobKeys) {
                String key = jobKey.getName().replace(TRIGGER_SUFFIX,"");
                AbstractJob job = abs.get(key);
                boolean isDelete = true;
                if(job!=null){
                    isDelete = job.isAutoOverwrite();
                }
                if(isDelete){
                    scheduler.deleteJob(jobKey);
                    log.info("auto manger clear job [{}]",jobKey);
                }
            }
        }catch (Exception e){
            e.printStackTrace();
        }
    }

    /**
     * 创建一个参数
     * @param beanName
     * @param job
     * @return
     */
    private void createJobDetail(String beanName,String detailBeanName,AbstractJob job){
        BeanDefinitionBuilder definitionBuilder = BeanDefinitionBuilder.genericBeanDefinition(JobDetailFactoryBean.class);
        definitionBuilder.addPropertyValue("jobClass",job.getClass());
        definitionBuilder.addPropertyValue("beanName",beanName);
        definitionBuilder.addPropertyValue("group",groupPrefix+job.group());
        definitionBuilder.addPropertyValue("durability",job.isComplateAfterDelete());
        definitionBuilder.addPropertyValue("description",job.descJob());
        definitionBuilder.addPropertyValue("requestsRecovery",job.isStartAutoRecovery());
        definitionBuilder.addPropertyValue("jobDataAsMap",job.initParam());
        defaultListableBeanFactory.registerBeanDefinition(detailBeanName,definitionBuilder.getBeanDefinition());
        log.info("success register jobdetail : [{}]",detailBeanName);
    }

    /**
     * 注册触发器
     * @param detail
     * @param beanName
     * @param job
     */
    private void createdTrigger(JobDetail detail,String beanName,AbstractJob job){
        String temp[] = job.triggerCron();
        String name = beanName+"Trigger";
        for (int i = 0; i < temp.length; i++) {
            String  triggerName = name+"_"+i;
            BeanDefinitionBuilder definitionBuilder = BeanDefinitionBuilder.genericBeanDefinition(CronTriggerFactoryBean.class);
            definitionBuilder.addPropertyValue("name",triggerName);
            definitionBuilder.addPropertyValue("group",groupPrefix+name);
            definitionBuilder.addPropertyValue("cronExpression",temp[i]);
            definitionBuilder.addPropertyValue("description",job.descTrigger());
            definitionBuilder.addPropertyValue("jobDetail",detail);
            defaultListableBeanFactory.registerBeanDefinition(triggerName,definitionBuilder.getBeanDefinition());
            try {
                scheduler.scheduleJob(detail, (Trigger) defaultListableBeanFactory.getBean(triggerName));
            }catch (Exception e){
                e.printStackTrace();
            }
            log.info("success register trigger : [{}]",triggerName);
        }
    }
}
```

